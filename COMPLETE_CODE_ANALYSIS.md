# Complete Code Analysis: What's Built vs What You Need

## üéì **AWS CONCEPTS EXPLAINED (For Learning)**

Before we dive in, let me explain the AWS services you're using:

### **API Gateway** (What it is)
- **Think of it as:** A front door to your AWS services
- **What it does:** Receives HTTP requests (like POST /webhook) and routes them to Lambda functions
- **Why use it:** Instead of exposing Lambda directly, API Gateway gives you:
  - HTTPS endpoints (secure)
  - CORS support (for web browsers)
  - Rate limiting
  - Request/response transformation
- **In your code:** `template.yaml` lines 237-322 create a REST API with `/webhook` endpoint

### **Lambda** (What it is)
- **Think of it as:** A function that runs in the cloud (serverless)
- **What it does:** Executes your Python code when triggered
- **Why use it:** No servers to manage, auto-scales, pay only for execution time
- **In your code:** 
  - `webhook_handler/lambda_function.py` - Receives webhook, sends to SQS
  - `orchestrator/lambda_function.py` - Processes messages, calls Bedrock AI

### **SQS** (Simple Queue Service) (What it is)
- **Think of it as:** A message queue (like a post office box)
- **What it does:** Stores messages temporarily, Lambda pulls messages when ready
- **Why use it:** Decouples webhook receiving from processing (webhook responds fast, processing happens async)
- **In your code:** `template.yaml` lines 32-54 creates queue + dead letter queue

### **DynamoDB** (What it is)
- **Think of it as:** A NoSQL database (like MongoDB, but AWS-managed)
- **What it does:** Stores your generated messages with fast lookups
- **Why use it:** Serverless, auto-scales, fast queries by email/status
- **In your code:** `template.yaml` lines 56-97 creates table with indexes

### **S3** (Simple Storage Service) (What it is)
- **Think of it as:** Cloud file storage (like Google Drive)
- **What it does:** Stores your brand guidelines JSON file
- **Why use it:** Cheap, reliable, versioned (can see file history)
- **In your code:** `template.yaml` lines 12-30 creates bucket

### **CloudWatch** (What it is)
- **Think of it as:** Monitoring and logging service
- **What it does:** Stores logs from Lambda, tracks metrics (approval rate, generation time)
- **Why use it:** Debug issues, monitor performance, set alarms
- **In your code:** Lambda functions publish metrics (lines 192-202, 346-362 in orchestrator)

### **SSM Parameter Store** (What it is)
- **Think of it as:** A secure key-value store for configuration
- **What it does:** Stores queue URLs, bucket names, model IDs (instead of hardcoding)
- **Why use it:** Change config without redeploying code, secure storage
- **In your code:** `template.yaml` lines 323-362 creates parameters

### **Bedrock** (What it is)
- **Think of it as:** AWS's AI service (like OpenAI, but AWS)
- **What it does:** Provides AI models (Nova Pro) to generate text
- **Why use it:** No need to train models, just call API with prompts
- **In your code:** `orchestrator/lambda_function.py` calls `bedrock_client.converse()` (lines 170-182, 316-328)

---

## ‚úÖ **WHAT'S ACTUALLY BUILT (Complete Inventory)**

### **1. Data Ingestion** ‚úÖ **PARTIALLY DONE**
**What exists:**
- ‚úÖ API Gateway webhook endpoint (`/webhook`)
- ‚úÖ Webhook handler Lambda that validates JSON
- ‚úÖ Sends to SQS queue
- ‚úÖ Basic error handling

**What's missing for "pipeline":**
- ‚ùå No Kinesis Data Streams (for high-volume streaming)
- ‚ùå No EventBridge (for event routing)
- ‚ùå No data transformation stage (just passes through)
- ‚ùå No batch processing capability
- ‚ùå No schema validation (accepts any JSON)

**Code evidence:**
- `webhook_handler/lambda_function.py` lines 49-69: Only checks if payload is dict, no schema validation
- `webhook_handler/lambda_function.py` lines 72-113: Just wraps payload and sends to SQS

---

### **2. AI Agent System** ‚úÖ **DONE (But Not a Registry)**
**What exists:**
- ‚úÖ Two AI agents: Generator + Judge
- ‚úÖ Both use Bedrock Nova Pro model
- ‚úÖ Retry logic with feedback loop
- ‚úÖ Agent configuration stored in SSM Parameter Store

**What's missing for "Agent Registry":**
- ‚ùå No DynamoDB table to store agent versions
- ‚ùå No way to register new agent versions
- ‚ùå No A/B testing (can't route X% to new agent)
- ‚ùå No agent metadata (created date, performance history)
- ‚ùå Hardcoded model ID in CloudFormation (line 8: `amazon.nova-pro-v1:0`)

**Code evidence:**
- `orchestrator/lambda_function.py` line 455: Gets model ID from SSM (single value, not versioned)
- `orchestrator/lambda_function.py` lines 110-213: Generator agent (hardcoded prompt)
- `orchestrator/lambda_function.py` lines 216-381: Judge agent (hardcoded prompt)
- No code to query agent registry or switch versions

---

### **3. Feature Store / Vector Database** ‚ùå **NOT BUILT**
**What exists:**
- ‚úÖ Customer data passed directly to AI (no feature store)
- ‚úÖ DynamoDB stores messages (but not features)

**What's missing:**
- ‚ùå No feature store (DynamoDB table for customer features)
- ‚ùå No vector database (OpenSearch/Bedrock Knowledge Bases)
- ‚ùå No customer embeddings (vector representations)
- ‚ùå No similar customer search
- ‚ùå No feature versioning

**Code evidence:**
- `orchestrator/lambda_function.py` lines 96-107: Just formats customer data as JSON string
- No code to query features or embeddings
- No OpenSearch/Bedrock Knowledge Bases in CloudFormation

---

### **4. Model Monitoring** ‚ö†Ô∏è **BASIC (Not Comprehensive)**
**What exists:**
- ‚úÖ CloudWatch metrics: `ApprovalRate`, `JudgeScore`, `GenerationTime`, `MessagesGenerated`, `RetryRate`
- ‚úÖ CloudWatch Logs: Lambda logs stored automatically
- ‚úÖ Basic metrics published (lines 192-202, 346-362, 546-562 in orchestrator)

**What's missing:**
- ‚ùå No CloudWatch Dashboard (visual monitoring)
- ‚ùå No data drift detection (SageMaker Model Monitor)
- ‚ùå No anomaly detection (unusual patterns)
- ‚ùå No SNS alerts (email/SMS when issues occur)
- ‚ùå No cost tracking (Bedrock API costs)
- ‚ùå No performance degradation detection

**Code evidence:**
- `orchestrator/lambda_function.py` lines 192-202: Publishes `GenerationTime` metric
- `orchestrator/lambda_function.py` lines 346-362: Publishes `ApprovalRate` and `JudgeScore`
- No dashboard creation in CloudFormation
- No SNS topics or alarms

---

### **5. Pipeline Services** ‚ö†Ô∏è **PARTIALLY DONE (Single Stage)**
**What exists:**
- ‚úÖ Inference stage (orchestrator Lambda processes messages)
- ‚úÖ Basic retry logic

**What's missing:**
- ‚ùå No Data Preparation stage (separate Lambda)
- ‚ùå No Training/Evaluation stage (for testing new agents)
- ‚ùå No Deployment stage (for rolling out new agents)
- ‚ùå All logic in one Lambda (not separated into stages)

**Code evidence:**
- `orchestrator/lambda_function.py` lines 436-569: Single `process_message()` function does everything
- No separate Lambdas for prep/eval/deploy stages
- No Step Functions or pipeline orchestration

---

### **6. Multiple AWS Accounts** ‚ùå **NOT BUILT**
**What exists:**
- ‚úÖ Single account deployment
- ‚úÖ CloudFormation template for one account

**What's missing:**
- ‚ùå No dev/test/prod account separation
- ‚ùå No AWS Organizations setup
- ‚ùå No cross-account IAM roles
- ‚ùå No deployment script that targets different accounts
- ‚ùå No account-specific configuration

**Code evidence:**
- `deploy.sh` lines 13-14: Hardcoded region `us-east-1`, no account parameter
- `template.yaml`: No account-specific resources
- No cross-account access patterns

---

### **7. Scalability Documentation** ‚ùå **NOT WRITTEN**
**What exists:**
- ‚úÖ Serverless architecture (auto-scales, but not documented)
- ‚úÖ DynamoDB on-demand (unlimited capacity)

**What's missing:**
- ‚ùå No written scalability plan
- ‚ùå No architecture diagrams showing scale
- ‚ùå No documentation of bottlenecks
- ‚ùå No plan for 10x/100x/1000x scale

---

### **8. Unique Service** ‚ùå **NOT IMPLEMENTED**
**What exists:**
- ‚úÖ Standard Bedrock API calls
- ‚úÖ Basic retry logic

**What's missing:**
- ‚ùå No Bedrock Knowledge Bases (RAG)
- ‚ùå No Bedrock Agents framework
- ‚ùå No Kinesis streaming
- ‚ùå No unique AWS service integration

---

## üìä **DETAILED BREAKDOWN BY REQUIREMENT**

### **Requirement 1: "Data ingestion through model monitoring"**

**Current State:**
```
Webhook ‚Üí API Gateway ‚Üí Lambda ‚Üí SQS ‚Üí Lambda ‚Üí Bedrock ‚Üí DynamoDB
```

**What you have:**
- ‚úÖ Data ingestion (webhook receives data)
- ‚úÖ Basic monitoring (CloudWatch metrics)

**What you need:**
- ‚ùå **Formal pipeline:** Add Kinesis Data Streams or EventBridge
- ‚ùå **Data transformation:** Separate Lambda for data prep
- ‚ùå **Monitoring integration:** Connect monitoring to ingestion pipeline

**AWS Learning:**
- **Kinesis Data Streams:** For high-volume real-time data ingestion (like Kafka)
- **EventBridge:** For event-driven architecture (routes events to different services)

---

### **Requirement 2: "Pipeline services: Data Preparation, Training, Evaluation, Deployment"**

**Current State:**
- Only **Inference** stage exists (orchestrator Lambda)

**What you need to build:**

#### **A. Data Preparation Stage**
**Create:** `aws/lambda/data_prep/lambda_function.py`
```python
# What it should do:
# 1. Validate customer data schema
# 2. Enrich with features (from Feature Store)
# 3. Create customer embeddings (vector representation)
# 4. Handle missing data
# 5. Output: Prepared data ready for AI
```

**AWS Learning:**
- **Lambda:** Serverless function (you already know this)
- **Step Functions:** Orchestrates multiple Lambdas in sequence (like a workflow)

#### **B. Training/Evaluation Stage** (For AI Agents)
**Create:** `aws/lambda/agent_evaluator/lambda_function.py`
```python
# What it should do:
# 1. Test new agent version on historical data
# 2. Compare performance (A/B test)
# 3. Generate evaluation report
# 4. Store results in Agent Registry
```

**AWS Learning:**
- **SageMaker:** For ML training/evaluation (but you're using AI agents, so adapt this)
- **DynamoDB:** Store evaluation results

#### **C. Deployment Stage**
**Create:** `aws/lambda/agent_deployer/lambda_function.py`
```python
# What it should do:
# 1. Register new agent version in Agent Registry
# 2. Update SSM parameters
# 3. Gradual rollout (10% ‚Üí 50% ‚Üí 100%)
# 4. Rollback if issues detected
```

**AWS Learning:**
- **SSM Parameter Store:** Already using, but need versioning
- **CloudWatch Alarms:** Trigger rollback if metrics degrade

#### **D. Enhanced Inference Stage**
**Current:** `orchestrator/lambda_function.py` (exists, but enhance)
**Enhancements:**
- Pull agent config from Agent Registry (not SSM)
- Use Feature Store for customer context
- Log detailed metrics

---

### **Requirement 3: "Model Registry, Feature Store, Model Monitoring, API Gateway"**

#### **A. Model Registry** (Agent Registry for you) ‚ùå **NOT BUILT**

**What to build:**
1. **DynamoDB Table:** `AgentRegistry`
   - Primary key: `agentId` (e.g., "upsell-generator")
   - Sort key: `version` (e.g., "2.1")
   - Attributes: `promptTemplate`, `bedrockModel`, `config`, `status`, `performance`

2. **Lambda Function:** `register_agent`
   - Registers new agent version
   - Validates configuration
   - Sets status (draft/testing/production)

3. **Update Orchestrator:**
   - Query Agent Registry for active version
   - Support A/B testing (route X% to new version)

**AWS Learning:**
- **DynamoDB:** You're already using it, just create another table
- **GSI (Global Secondary Index):** Query by `status` to find production agents

**Code to add:**
```python
# In orchestrator/lambda_function.py, replace line 455:
# OLD: model_id = get_parameter('/referral-system/bedrock-model-id')
# NEW:
agent_registry = dynamodb.Table('AgentRegistry')
active_agent = agent_registry.query(
    IndexName='StatusIndex',
    KeyConditionExpression='agentId = :id AND #status = :status',
    ExpressionAttributeNames={'#status': 'status'},
    ExpressionAttributeValues={':id': 'upsell-generator', ':status': 'production'}
)['Items'][0]
model_id = active_agent['bedrockModel']
prompt_template = active_agent['promptTemplate']
```

#### **B. Feature Store** ‚ùå **NOT BUILT**

**What to build:**
1. **DynamoDB Table:** `CustomerFeatures`
   - Primary key: `customerEmail`
   - Attributes: `satisfaction_avg`, `service_count`, `lifetime_value`, `property_features`, `embeddings` (vector)

2. **Lambda Function:** `feature_enricher`
   - Calculates features from customer data
   - Stores in Feature Store
   - Updates when new service completed

3. **Vector Database (Optional but recommended):**
   - **Amazon OpenSearch** or **Bedrock Knowledge Bases**
   - Store customer embeddings for similarity search
   - Find similar customers for personalization

**AWS Learning:**
- **OpenSearch:** Managed Elasticsearch (for vector search)
- **Bedrock Knowledge Bases:** AWS-managed RAG (Retrieval Augmented Generation)
- **Vector embeddings:** Numerical representation of customer (for similarity)

**Code to add:**
```python
# In data_prep Lambda:
# Calculate features
features = {
    'customerEmail': customer['email'],
    'satisfaction_avg': sum(s['satisfaction_score'] for s in service_history) / len(service_history),
    'service_count': len(service_history),
    'lifetime_value': calculate_lifetime_value(customer),
    'property_features': property_info.get('lot_features', [])
}

# Store in Feature Store
feature_store.put_item(Item=features)

# Create embedding (using Bedrock Titan Embeddings)
embedding = bedrock_client.invoke_model(
    modelId='amazon.titan-embed-text-v1',
    body=json.dumps({'inputText': json.dumps(features)})
)['embedding']

# Store in vector DB (OpenSearch)
opensearch.index(
    index='customers',
    body={'customerEmail': customer['email'], 'embedding': embedding}
)
```

#### **C. Model Monitoring** ‚ö†Ô∏è **BASIC (Needs Enhancement)**

**What exists:**
- ‚úÖ Basic CloudWatch metrics

**What to add:**
1. **CloudWatch Dashboard**
   - Visual charts for approval rate, latency, errors
   - Cost tracking (Bedrock API calls)

2. **CloudWatch Alarms**
   - Alert if approval rate drops below 70%
   - Alert if latency exceeds threshold
   - Alert if error rate spikes

3. **SNS Topics**
   - Email/SMS notifications when alarms trigger

4. **Data Drift Detection** (Advanced)
   - SageMaker Model Monitor (detects if customer data distribution changes)
   - Or custom Lambda that compares current vs historical distributions

**AWS Learning:**
- **CloudWatch Dashboard:** Visual monitoring (like Grafana)
- **CloudWatch Alarms:** Triggers actions when metrics cross thresholds
- **SNS (Simple Notification Service):** Sends emails/SMS/Slack messages

**Code to add:**
```yaml
# In template.yaml, add:
CloudWatchDashboard:
  Type: AWS::CloudWatch::Dashboard
  Properties:
    DashboardName: AgentMonitoringDashboard
    DashboardBody: |
      {
        "widgets": [
          {
            "type": "metric",
            "properties": {
              "metrics": [
                ["ReferralSystem", "ApprovalRate"],
                [".", "JudgeScore"]
              ],
              "period": 300,
              "stat": "Average",
              "region": "us-east-1",
              "title": "Agent Performance"
            }
          }
        ]
      }

SNSAlarmTopic:
  Type: AWS::SNS::Topic
  Properties:
    TopicName: agent-alarms
    Subscription:
      - Protocol: email
        Endpoint: your-email@example.com

ApprovalRateAlarm:
  Type: AWS::CloudWatch::Alarm
  Properties:
    AlarmName: LowApprovalRate
    MetricName: ApprovalRate
    Namespace: ReferralSystem
    Statistic: Average
    Period: 300
    EvaluationPeriods: 2
    Threshold: 0.7
    ComparisonOperator: LessThanThreshold
    AlarmActions:
      - !Ref SNSAlarmTopic
```

#### **D. API Gateway** ‚úÖ **ALREADY BUILT**
- ‚úÖ REST API with `/webhook` endpoint
- ‚úÖ CORS configured
- ‚úÖ Lambda integration

**No changes needed!**

---

### **Requirement 4: "Multiple AWS Accounts"** ‚ùå **NOT BUILT**

**What to build:**

1. **Create 3 AWS Accounts:**
   - Dev account (for development)
   - Test account (for staging)
   - Prod account (for production)

2. **AWS Organizations** (Optional but recommended)
   - Centralized billing
   - Account management

3. **Cross-Account IAM Roles**
   - Allow CI/CD to deploy across accounts
   - Allow monitoring to aggregate from all accounts

4. **Update Deployment Script**
   - Add `--account` parameter
   - Assume cross-account role
   - Deploy to specified account

**AWS Learning:**
- **AWS Organizations:** Manages multiple AWS accounts
- **IAM Cross-Account Roles:** Allows one account to access another
- **AssumeRole:** Temporary credentials to access another account

**Code to add:**
```bash
# In deploy.sh, add:
ACCOUNT=${1:-dev}  # Default to dev

case $ACCOUNT in
  dev)
    AWS_ACCOUNT_ID="111111111111"
    ROLE_ARN="arn:aws:iam::111111111111:role/DeploymentRole"
    ;;
  test)
    AWS_ACCOUNT_ID="222222222222"
    ROLE_ARN="arn:aws:iam::222222222222:role/DeploymentRole"
    ;;
  prod)
    AWS_ACCOUNT_ID="333333333333"
    ROLE_ARN="arn:aws:iam::333333333333:role/DeploymentRole"
    ;;
esac

# Assume role
CREDENTIALS=$(aws sts assume-role \
  --role-arn $ROLE_ARN \
  --role-session-name deploy-session)

export AWS_ACCESS_KEY_ID=$(echo $CREDENTIALS | jq -r '.Credentials.AccessKeyId')
export AWS_SECRET_ACCESS_KEY=$(echo $CREDENTIALS | jq -r '.Credentials.SecretAccessKey')
export AWS_SESSION_TOKEN=$(echo $CREDENTIALS | jq -r '.Credentials.SessionToken')

# Deploy
aws cloudformation deploy --stack-name referral-email-system-$ACCOUNT ...
```

---

### **Requirement 5: "Scalability Documentation"** ‚ùå **NOT WRITTEN**

**What to write:**

1. **Current Architecture Capacity:**
   - Lambda: 1000 concurrent executions (default)
   - API Gateway: 10,000 requests/second (default)
   - DynamoDB: On-demand (unlimited)
   - SQS: Unlimited messages

2. **Scaling Strategies:**
   - **10x scale:** Current architecture (no changes)
   - **100x scale:** Add ElastiCache, CloudFront, DynamoDB read replicas
   - **1000x scale:** Multi-region, Kinesis Data Streams, ECS/Fargate

3. **Bottlenecks:**
   - Bedrock API rate limits (check AWS docs)
   - Lambda cold starts (mitigate with provisioned concurrency)
   - DynamoDB throttling (on-demand handles this)

**Create:** `SCALABILITY_PLAN.md`

---

### **Requirement 6: "Unique Service/Approach"** ‚ùå **NOT IMPLEMENTED**

**Recommended: Bedrock Knowledge Bases (RAG)**

**What to build:**

1. **Bedrock Knowledge Base:**
   - Data source: S3 bucket with customer service history
   - Embeddings: Amazon Titan Embeddings
   - Vector store: OpenSearch Serverless

2. **Enhance Orchestrator:**
   - Before generating message, query Knowledge Base
   - Find similar customers and successful upsells
   - Include examples in prompt

**AWS Learning:**
- **RAG (Retrieval Augmented Generation):** AI retrieves relevant context before generating
- **Knowledge Bases:** Managed RAG by AWS
- **Vector search:** Find similar items using embeddings

**Code to add:**
```python
# In orchestrator/lambda_function.py, before line 474:

# Query Knowledge Base for similar customers
knowledge_base_id = get_parameter('/referral-system/knowledge-base-id')
similar_cases = bedrock_agent_client.retrieve(
    knowledgeBaseId=knowledge_base_id,
    retrievalQuery=f"Customer with {customer_data['property_info']['type']} and {len(service_history)} services",
    retrievalConfiguration={
        'vectorSearchConfiguration': {
            'numberOfResults': 3
        }
    }
)

# Include similar cases in prompt
similar_cases_text = "\n\nSimilar successful upsells:\n"
for case in similar_cases['retrievalResults']:
    similar_cases_text += f"- {case['content']['text']}\n"

# Add to prompt (line 128)
prompt = f"""...{similar_cases_text}"""
```

---

## üìã **FINAL CHECKLIST: What You Need to Build**

### **Priority 1: Core Requirements (Must Have)**

- [ ] **Agent Registry** (DynamoDB table + Lambda to register)
- [ ] **Feature Store** (DynamoDB table + Lambda to calculate features)
- [ ] **CloudWatch Dashboard** (Visual monitoring)
- [ ] **Data Preparation Stage** (Separate Lambda)
- [ ] **Multi-Account Setup** (3 accounts + deployment script)

### **Priority 2: Enhanced Requirements (Should Have)**

- [ ] **Agent Evaluator Lambda** (Test new agents)
- [ ] **Agent Deployer Lambda** (Rollout new versions)
- [ ] **CloudWatch Alarms + SNS** (Alerting)
- [ ] **Scalability Documentation** (Written plan)

### **Priority 3: Unique Feature (Nice to Have)**

- [ ] **Bedrock Knowledge Bases** (RAG for similar customer search)
- [ ] **Vector Database** (OpenSearch for embeddings)

---

## üéØ **RECOMMENDED IMPLEMENTATION ORDER**

1. **Week 1:** Agent Registry + Feature Store (foundation)
2. **Week 2:** CloudWatch Dashboard + Alarms (monitoring)
3. **Week 3:** Data Prep + Evaluator + Deployer Lambdas (pipeline)
4. **Week 4:** Multi-Account Setup (dev/test/prod)
5. **Week 5:** Bedrock Knowledge Bases (unique feature) + Documentation

---

## üí° **QUICK START: Build Agent Registry First**

This is the easiest and most impactful. Here's what to do:

1. **Add DynamoDB table to CloudFormation:**
```yaml
AgentRegistryTable:
  Type: AWS::DynamoDB::Table
  Properties:
    TableName: AgentRegistry
    BillingMode: PAY_PER_REQUEST
    AttributeDefinitions:
      - AttributeName: agentId
        AttributeType: S
      - AttributeName: version
        AttributeType: S
      - AttributeName: status
        AttributeType: S
    KeySchema:
      - AttributeName: agentId
        KeyType: HASH
      - AttributeName: version
        KeyType: RANGE
    GlobalSecondaryIndexes:
      - IndexName: StatusIndex
        KeySchema:
          - AttributeName: status
            KeyType: HASH
          - AttributeName: version
            KeyType: RANGE
        Projection:
          ProjectionType: ALL
```

2. **Create Lambda to register agents:**
```python
# aws/lambda/register_agent/lambda_function.py
import boto3
import json
from datetime import datetime

dynamodb = boto3.resource('dynamodb')
table = dynamodb.Table('AgentRegistry')

def lambda_handler(event, context):
    agent_id = event['agentId']
    version = event['version']
    prompt_template = event['promptTemplate']
    bedrock_model = event['bedrockModel']
    
    table.put_item(Item={
        'agentId': agent_id,
        'version': version,
        'promptTemplate': prompt_template,
        'bedrockModel': bedrock_model,
        'status': 'draft',  # draft/testing/production
        'createdAt': datetime.utcnow().isoformat(),
        'performance': {}
    })
    
    return {'statusCode': 200, 'body': json.dumps('Agent registered')}
```

3. **Update orchestrator to use registry:**
```python
# In orchestrator/lambda_function.py, replace model_id lookup:
agent_registry = dynamodb.Table('AgentRegistry')
active_agent = agent_registry.query(
    IndexName='StatusIndex',
    KeyConditionExpression='#status = :status',
    ExpressionAttributeNames={'#status': 'status'},
    ExpressionAttributeValues={':status': 'production'},
    ScanIndexForward=False,
    Limit=1
)['Items'][0]

model_id = active_agent['bedrockModel']
```

This gives you versioned agents! üéâ

---

## üìö **AWS LEARNING RESOURCES**

- **AWS Well-Architected Framework:** Best practices for building on AWS
- **AWS Serverless Application Model (SAM):** Easier CloudFormation
- **AWS Step Functions:** Orchestrate Lambda workflows
- **AWS Bedrock Documentation:** AI/ML services

---

This analysis is complete and accurate based on reading ALL your code. You have a solid foundation, but need to add the pipeline stages, registry, feature store, and multi-account setup to meet final project requirements.

